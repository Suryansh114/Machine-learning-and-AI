# -*- coding: utf-8 -*-
"""numericdetector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oa_47CgG0U2uaQcLE1eQOHKf8l2xjZf2
"""

!pip install jovian
import jovian
jovian.utils.colab.set_colab_file_id('eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcmVzaCI6ZmFsc2UsImlhdCI6MTcwNTMyMjIyNiwianRpIjoiYjBjOTU1ZWUtNDViNi00YTVjLThlZTctZmQ4ODBiNDVmNDQzIiwidHlwZSI6ImFjY2VzcyIsImlkZW50aXR5Ijp7ImlkIjozNzM5MTQsInVzZXJuYW1lIjoic3VyeWFuc2gxMTQyMDA0In0sIm5iZiI6MTcwNTMyMjIyNiwiZXhwIjoxNzA5MjEwMjI2fQ.3EJKzCQFgSdvZP5g1xemZ2bomOyp2R6I4ROXXTy8sfg')

import torch
import torchvision
from torchvision.datasets import MNIST

dataset= MNIST(root='data/', download=True)

len(dataset)

test_dataset=MNIST(root='data/',train=False)
len(test_dataset)
#test set

dataset[0]

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

image,label = dataset[0]
plt.imshow(image,cmap='gray')
print('label: ',label )

image,label = dataset[10]
plt.imshow(image,cmap='gray')
print('label: ',label )

import torchvision.transforms as transform

dataset=MNIST(root='data/',
              train=True,
              transform = transform.ToTensor())

img_tensor, label =dataset[0]
print(img_tensor.shape,label)

print(img_tensor[:,10:15,10:15])
print(torch.max(img_tensor),torch.min(img_tensor))
# 0 value represent black and the non 0 value represents the different shades of gray

plt.imshow(img_tensor[0,10:15,10:15],cmap='gray')
#plotting the above dimensions on a map will look like

#Dividing data into training and validation set
from torch.utils.data import random_split
train_ds, val_ds = random_split (dataset,[50000,10000])
len(train_ds),len(val_ds)

from torch.utils.data import DataLoader

batch_size =128

train_loader = DataLoader (train_ds, batch_size , shuffle=True)
val_loader = DataLoader(val_ds, batch_size)

import torch.nn as nn
input_size = 28*28 # this sis the resolution that is 784

num_classes=10

model=nn.Linear(input_size, num_classes)

print(model.weight.shape)
model.weight

"""print(model.bias.shape)
model.bias
"""

for images,labels in train_loader:
  print(labels)
  print(images.shape)
  #outputs =model(images)
  #now the torch library expects the images and label to be of two dimension array but as we can see from the shampe of images which is in 4 D so we need to convert the shape into a 2 D array using the reshape function
  break

images.shape

images.reshape(128,784).shape

#defining an object in python

class Person:
    def __init__(self, name, age):  #the self argument should be always prsent in order to define the object
        self.name = name
        self.age = age

    def say_hello(self):
        print("Hello my name is " + self.name + "!")

bob = Person("Bob",32)

bob.name,bob.age

bob.say_hello

class MnistModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear = nn.Linear(input_size, num_classes)

  def forward(self,xb):
    xb = xb.reshape(-1,784)
    out = self.linear(xb)
    return out

model = MnistModel()

model.linear

print(model.linear.weight.shape, model.linear.bias.shape)
list(model.parameters())

for images,labels in train_loader:
  outputs = model(images)
  break
print('output.shape: ',outputs.shape)
print('sample outputs : \n',outputs[:2].data)

#softmax function = S(yi)=e^yi/sum(eyj)

import torch.nn.functional as F

probs =F.softmax(outputs,dim=1)
print("sample probabilities: \n",probs[:2].data)
print("sum: ",torch.sum(probs[0]).item())

max_prob, preds= torch.max(probs,dim=1)
print(preds)
print(max_prob)

#now we apply the gradient descnet

def accuracy(outputs,labels):
  _, preds = torch.max(outputs,dim=1)
  return torch.tensor(torch.sum(preds == labels).item()/len(preds))
  #the == sign gives back values in true and false.

accuracy(outputs, labels)
#now this very small

#so now we will use cross entrpy

probs

loss_fn = F.cross_entropy
loss=loss_fn(outputs,labels)
print(loss)

def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    optimizer = opt_func(model.parameters(), lr)
    history = [] # for recording epoch-wise results

    for epoch in range(epochs):

        # Training Phase
        for batch in train_loader:
            loss = model.training_step(batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        # Validation phase
        result = evaluate(model, val_loader)
        model.epoch_end(epoch, result)
        history.append(result)

    return history

def evaluate(model, val_loader):
  outputs= [model.validation_step(batch) for batch in val_loader]
  return model.validation_epoch_end(outputs)
  #line 2 is used to fo operations in a list as a whole
  #validation_epoch_end is used to average the loss

class MnistModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(input_size, num_classes)

    def forward(self, xb):
        xb = xb.reshape(-1, 784)
        out = self.linear(xb)
        return out

    def training_step(self, batch):
        images, labels = batch
        out = self(images)                  # Generate predictions
        loss = F.cross_entropy(out, labels) # Calculate loss
        return loss

    def validation_step(self, batch):
        images, labels = batch
        out = self(images)                    # Generate predictions
        loss = F.cross_entropy(out, labels)   # Calculate loss
        acc = accuracy(out, labels)           # Calculate accuracy
        return {'val_loss': loss, 'val_acc': acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}".format(epoch, result['val_loss'], result['val_acc']))

model = MnistModel()

result0= evaluate(model,val_loader)
result0

history1 = fit(5, 0.001,model,train_loader,val_loader )

history3 = fit(5, 0.001,model,train_loader,val_loader )

history2 = fit(5, 0.001,model,train_loader,val_loader )

history = [result0]+history1 + history3 + history2
accuracies = [result['val_acc'] for result in history]
plt.plot(accuracies, '-x')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.title('Accuracy vs No. of epoch')

jovian.log_metrics(val_acc = history[-1]['val_acc'],val_loss = history[-1]['val_loss'])

!pip install torch torchvision

from torchvision import datasets, transforms

# Define test dataset
test_dataset = MNIST(root='data/',
                     train=False,
                     transform=transforms.ToTensor())

img, label = test_dataset[0]
plt.imshow(img[0], cmap ='gray')
print('shape: ',img.shape)
print('label: ', label)

def predict_image (img,model):
  xb = img.unsqueeze(0)
  yb= model(xb)
  _, preds = torch.max(yb, dim=1)
  return preds[0].item()

img.unsqueeze(0).shape
#the model cannot take in values multiple values like 28*28 so we unsqueeze it adding another dimension which the model views as a single batch

img, label = test_dataset[0]
plt.imshow(img[0],cmap='gray')
print('label: ', label, ',predictions', predict_image(img, model))

img, label = test_dataset[10]
plt.imshow(img[0],cmap='gray')
print('label: ', label, ',predictions', predict_image(img, model))

img, label = test_dataset[193]
plt.imshow(img[0],cmap='gray')
print('label: ', label, ',predictions', predict_image(img, model))
#sometimes the model doesnt work due to the lament pixel and placement

img, label = test_dataset[1839]
plt.imshow(img[0],cmap='gray')
print('label: ', label, ',predictions', predict_image(img, model))

test_loader = DataLoader(test_dataset, batch_size = 256)
result = evaluate(model, test_loader)
result

